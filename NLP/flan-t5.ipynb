{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":207.755599,"end_time":"2024-01-24T06:54:52.961290","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-24T06:51:25.205691","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# flan-T5 LLM model\n\nSteps included\n1. Load flan-t5 model & dialogue-summarization dataset.\n2. Full fine-tune flan-T5 model on GPU\n3. Test inference of Base model and Fine-tuned model\n4. Test the fine-tuned model with rough and bleu scores\n5. Track the experiment with wandb (weights and biases)","metadata":{"papermill":{"duration":0.006823,"end_time":"2024-01-24T06:51:28.773059","exception":false,"start_time":"2024-01-24T06:51:28.766236","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    wandb \\\n    peft==0.3.0 --quiet","metadata":{"papermill":{"duration":129.079385,"end_time":"2024-01-24T06:53:37.859342","exception":false,"start_time":"2024-01-24T06:51:28.779957","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:50:29.856008Z","iopub.execute_input":"2024-05-01T12:50:29.856424Z","iopub.status.idle":"2024-05-01T12:53:13.945867Z","shell.execute_reply.started":"2024-05-01T12:50:29.856395Z","shell.execute_reply":"2024-05-01T12:53:13.944774Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.27.2 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.6 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.14 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"papermill":{"duration":11.735744,"end_time":"2024-01-24T06:53:49.603073","exception":false,"start_time":"2024-01-24T06:53:37.867329","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:53:13.948097Z","iopub.execute_input":"2024-05-01T12:53:13.948391Z","iopub.status.idle":"2024-05-01T12:53:26.442783Z","shell.execute_reply.started":"2024-05-01T12:53:13.948363Z","shell.execute_reply":"2024-05-01T12:53:26.441781Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.11.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/datasets#egg=datasets","metadata":{"papermill":{"duration":29.014135,"end_time":"2024-01-24T06:54:18.625780","exception":false,"start_time":"2024-01-24T06:53:49.611645","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:53:26.444159Z","iopub.execute_input":"2024-05-01T12:53:26.444482Z","iopub.status.idle":"2024-05-01T12:53:56.230955Z","shell.execute_reply.started":"2024-05-01T12:53:26.444443Z","shell.execute_reply":"2024-05-01T12:53:56.229934Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting datasets\n  Cloning https://github.com/huggingface/datasets to /tmp/pip-install-70ld035t/datasets_0d860a46be254ecfbb6c2c840181ffa3\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/datasets /tmp/pip-install-70ld035t/datasets_0d860a46be254ecfbb6c2c840181ffa3\n  Resolved https://github.com/huggingface/datasets to commit ceb25e118f21f54b5b5c5e9c223713f14a798eb5\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nBuilding wheels for collected packages: datasets\n  Building wheel for datasets (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for datasets: filename=datasets-2.19.1.dev0-py3-none-any.whl size=517668 sha256=a0bdb8531eaf004d784624ddc47ff37900157eeff2b1e47b9fbafbf21f3031e9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ndsnwe7e/wheels/7f/ba/ce/8f6a52388a9966c7d9afa987113a763f7c105f568f369adbc6\nSuccessfully built datasets\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.11.0\n    Uninstalling datasets-2.11.0:\n      Successfully uninstalled datasets-2.11.0\nSuccessfully installed datasets-2.19.1.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"papermill":{"duration":11.739989,"end_time":"2024-01-24T06:54:30.376877","exception":false,"start_time":"2024-01-24T06:54:18.636888","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:53:56.232355Z","iopub.execute_input":"2024-05-01T12:53:56.232677Z","iopub.status.idle":"2024-05-01T12:54:08.760887Z","shell.execute_reply.started":"2024-05-01T12:53:56.232648Z","shell.execute_reply":"2024-05-01T12:54:08.759802Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import datasets \nprint(datasets.__version__)","metadata":{"papermill":{"duration":1.931371,"end_time":"2024-01-24T06:54:32.320530","exception":false,"start_time":"2024-01-24T06:54:30.389159","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:54:08.763226Z","iopub.execute_input":"2024-05-01T12:54:08.763587Z","iopub.status.idle":"2024-05-01T12:54:10.065440Z","shell.execute_reply.started":"2024-05-01T12:54:08.763556Z","shell.execute_reply":"2024-05-01T12:54:10.064446Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"2.19.1.dev0\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, EarlyStoppingCallback\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nimport wandb\n\n# wandb.login() ","metadata":{"papermill":{"duration":17.74721,"end_time":"2024-01-24T06:54:50.078956","exception":true,"start_time":"2024-01-24T06:54:32.331746","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:54:10.066678Z","iopub.execute_input":"2024-05-01T12:54:10.067078Z","iopub.status.idle":"2024-05-01T12:54:23.427530Z","shell.execute_reply.started":"2024-05-01T12:54:10.067054Z","shell.execute_reply":"2024-05-01T12:54:23.426607Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-05-01 12:54:13.835998: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-01 12:54:13.836106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-01 12:54:13.944745: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1. Load the flan-t5 model and dialogue summarization dataset\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"huggingface_dataset_name = \"knkarthick/dialogsum\"\ndataset = load_dataset(huggingface_dataset_name)\n\n#load model and tokenzier\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\ntokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n\ndtype = next(original_model.parameters()).dtype\nprint(f\"Tensor's dataType -->{dtype}\")\n\n#check where the model is loaded (should print either cpu or cuda)\nprint(f\"Model is loaded on -->{next(original_model.parameters()).device}\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:54:23.429225Z","iopub.execute_input":"2024-05-01T12:54:23.429526Z","iopub.status.idle":"2024-05-01T12:54:35.301110Z","shell.execute_reply.started":"2024-05-01T12:54:23.429479Z","shell.execute_reply":"2024-05-01T12:54:35.300145Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0757e78a0b3441caa044a4b70c46b881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959daf78d3fb4287a75a5216dcf4b78f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c74e49e5a0a4d38925629a919545c14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba5be23db5c24421bdf40cc06e355e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c860888b3d04b4990967b43907c4cf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb48dc474fd4a2d8140f4c5700d3187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2872502e841415383409b96a4ab5008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0e565d8ac9c4a0798a7665649781481"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36aa7ef4928b4c9f978781253573f94d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aac68fa790e4f8896dac04916bedb51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67bc4d24dbd14048b02d844682144309"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e3830fc9bb4dc588546bf324f7d3a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34162f45eef1460ca9386adba7daf800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c10473bf2e43d1ae576fac44bd2630"}},"metadata":{}},{"name":"stdout","text":"Tensor's dataType -->torch.float32\nModel is loaded on -->cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1.3 Redoing the datasplits for balalanced & optimum test/validation/test split ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets, DatasetDict\n\n# Combine the splits (train, test, validation)\ncombined_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]])\n\n# Shuffle the combined dataset\ncombined_dataset = combined_dataset.shuffle(seed=42)\n\n# Split the dataset into 80% train, 10% test, 10% validation\ntrain_test_split = combined_dataset.train_test_split(test_size=0.20)  # Splitting 20% for test+validation\ntest_validation_split = train_test_split['test'].train_test_split(test_size=0.5)  # Splitting the 20% into two equal halves\n\n# Creating the final DatasetDict\nfinal_dataset = DatasetDict({\n    'train': train_test_split['train'],\n    'test': test_validation_split['test'],\n    'validation': test_validation_split['train']\n})\n\ntest_summaries = final_dataset['test']['summary']","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:54:35.302574Z","iopub.execute_input":"2024-05-01T12:54:35.302872Z","iopub.status.idle":"2024-05-01T12:54:35.379958Z","shell.execute_reply.started":"2024-05-01T12:54:35.302848Z","shell.execute_reply":"2024-05-01T12:54:35.378958Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 1.4 Tokenizing the dataset for training","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# Compress the given text to short expressions, and such that you can reconstruct it \n# as close as possible to the original. Unlike the usual text compression, \n# I need you to comply with the 5 conditions below:\n    \n# 1. You can ONLY remove unimportant words. \n# 2. Do not reorder the original words.\n# 3. Do not change the original words.\n# 4. Do not use abbreviations or emojis.\n# 5. Do not add new words or symbols.\n\n# Compress the origin aggressively by removing words only. \n# Compress the origin as short as you can, while retain- ing as much information as possible. \n# If you understand, please compress the following text: {text to compress} \n# The compressed text is:\n\ndef tokenize_function(examples):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompts = [start_prompt + dialogue + end_prompt for dialogue in examples[\"dialogue\"]]\n    model_max_input_length = tokenizer.model_max_length\n\n    # Tokenize the input dialogue text\n    tokenized_inputs = tokenizer(prompts, max_length=model_max_input_length, padding=\"max_length\", truncation=True)\n    \n    # Tokenize the labels for the dialogues\n    tokenized_labels = tokenizer(examples[\"summary\"], max_length=model_max_input_length, padding=\"max_length\", truncation=True)\n\n    # We need to replace the labels token ids of padding with -100 so they are not taken into account in the loss computation\n    tokenized_labels[\"input_ids\"] = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in labels] for labels in tokenized_labels[\"input_ids\"]\n    ]\n\n    return {\"input_ids\": tokenized_inputs[\"input_ids\"], \"labels\": tokenized_labels[\"input_ids\"]}\n\ntokenized_datasets = final_dataset.map(tokenize_function, batched=True)\n\n# Remove columns which are not necessary for training\ncolumns_to_remove = ['id', 'topic', 'dialogue', 'summary']\ntokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:54:38.528649Z","iopub.execute_input":"2024-05-01T12:54:38.529264Z","iopub.status.idle":"2024-05-01T12:54:58.966839Z","shell.execute_reply.started":"2024-05-01T12:54:38.529231Z","shell.execute_reply":"2024-05-01T12:54:58.966109Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11568 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd4a05028624418387f8205694d565db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a6954036b114192a5c024e0fc03d842"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2423652dbf9d4d2181846ce8d1c5bbf4"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 2 Full-finetune the flan-t5 model by training with above dataset & track experiment with wandb","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"lr_rate = 3e-5\nwt_decay = 0.01\nearly_st_th = 0.009 \nearly_st_ptnce = 3\nsteps = 250\n\n# wandb configuration for experiment tracking\nconfig={\n    'learning_rate': lr_rate,\n    'weight_decay': wt_decay,\n    'early_stopping_threshold' : early_st_th,\n    'early_stopping_patience':early_st_ptnce,\n    'steps':steps,\n    'per_device_train_batch_size':32,\n    'per_device_eval_batch_size':16,\n}\n\ntimestamp = str(int(time.time()))\n\noutput_dir = f'/notebooks/models/flant5-fullfinetuned-{timestamp}'\n\n# early stopping callback will help to stop the training if no siginficant reduction in error is observed.\nearly_stopping_callback = EarlyStoppingCallback(early_stopping_patience=early_st_ptnce, early_stopping_threshold=early_st_th)\n\ntraining_args = TrainingArguments(\n    report_to= \"wandb\",\n    output_dir=output_dir,\n    learning_rate=lr_rate,\n    auto_find_batch_size=True,\n    weight_decay=wt_decay,\n    logging_steps=steps,\n    eval_steps=steps,\n    max_steps=1000,\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end = True,\n    gradient_accumulation_steps=2,   \n    max_grad_norm=1.0,\n    warmup_steps=500, \n)\n\ntrainer = Trainer(\n    model=original_model.to(\"cuda:0\"),\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    callbacks=[early_stopping_callback]\n)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:55:12.599846Z","iopub.execute_input":"2024-05-01T12:55:12.600242Z","iopub.status.idle":"2024-05-01T12:55:14.090417Z","shell.execute_reply.started":"2024-05-01T12:55:12.600212Z","shell.execute_reply":"2024-05-01T12:55:14.089546Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(project='genai-llm', config=config, name=f'flant5-fullfinetune-{timestamp}')\nstart_time = time.time()\ntrainer.train()\ntraining_time = time.time() - start_time\nrun.log({\"Training time (seconds)\":training_time})\nrun.log({\"Training configuration\":config})","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T12:55:30.827949Z","iopub.execute_input":"2024-05-01T12:55:30.828644Z","iopub.status.idle":"2024-05-01T13:33:21.589019Z","shell.execute_reply.started":"2024-05-01T12:55:30.828609Z","shell.execute_reply":"2024-05-01T13:33:21.587752Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111350547777824, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196fbd1716b946f0beb2d313c2c998b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240501_125617-msfmx24f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mann1105/genai-llm/runs/msfmx24f' target=\"_blank\">flant5-fullfinetune-1714568112</a></strong> to <a href='https://wandb.ai/mann1105/genai-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mann1105/genai-llm' target=\"_blank\">https://wandb.ai/mann1105/genai-llm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mann1105/genai-llm/runs/msfmx24f' target=\"_blank\">https://wandb.ai/mann1105/genai-llm/runs/msfmx24f</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1000 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 36:36, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>1.559200</td>\n      <td>1.167345</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.287800</td>\n      <td>1.111439</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.233300</td>\n      <td>1.090827</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.232700</td>\n      <td>1.084353</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"# save the best model and tokenizer\ntrainer.save_model(f\"{output_dir}/final\")\ntokenizer.save_pretrained(f\"{output_dir}/final\")\n\nmodel_artifact = wandb.Artifact('model_artifact', type='model')\nmodel_artifact.add_dir(f\"{output_dir}/final\")\nrun.log_artifact(model_artifact)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T13:33:21.590996Z","iopub.execute_input":"2024-05-01T13:33:21.591299Z","iopub.status.idle":"2024-05-01T13:33:33.918729Z","shell.execute_reply.started":"2024-05-01T13:33:21.591272Z","shell.execute_reply":"2024-05-01T13:33:33.917439Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/notebooks/models/flant5-fullfinetuned-1714568112/final)... Done. 10.7s\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<Artifact model_artifact>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3. Now let's compare the inference of the original and the fine-tuned model with zero shot prompt","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"## load the new model and tokenizer\nfinetuned_model = AutoModelForSeq2SeqLM.from_pretrained(f\"{output_dir}/final\")\ntokenizer2 = AutoTokenizer.from_pretrained(f\"{output_dir}/final\")\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\ntokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T13:33:33.919955Z","iopub.execute_input":"2024-05-01T13:33:33.920317Z","iopub.status.idle":"2024-05-01T13:33:41.122161Z","shell.execute_reply.started":"2024-05-01T13:33:33.920282Z","shell.execute_reply":"2024-05-01T13:33:41.120954Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#let's get inference from original model\nexample_record = 200\ndialogue = dataset['test'][example_record]['dialogue']\n\nprint(dialogue)\n\nstart_prompt = 'Summarize the following conversation.\\n\\n'\nend_prompt = '\\n\\nSummary: '\nprompt = start_prompt + dialogue + end_prompt\n\n\ninput = tokenizer(prompt, return_tensors='pt')\noutput_tokens = original_model.generate(input[\"input_ids\"], max_new_tokens=50,)\noriginal_model_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n\nprint(\"Summary-->\")\nprint(original_model_output)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T13:33:41.125390Z","iopub.execute_input":"2024-05-01T13:33:41.125801Z","iopub.status.idle":"2024-05-01T13:33:46.652553Z","shell.execute_reply.started":"2024-05-01T13:33:41.125768Z","shell.execute_reply":"2024-05-01T13:33:46.651563Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\nSummary-->\n#Person1#: I'm thinking of upgrading my computer.\n","output_type":"stream"}]},{"cell_type":"code","source":"#lets get inference from finetuned model\n\ninput = tokenizer2(prompt, return_tensors='pt')\noutput_tokens = finetuned_model.generate(input[\"input_ids\"], max_new_tokens=50,)\nfinetuned_model_output = tokenizer2.decode(output_tokens[0], skip_special_tokens=True)\n\nprint(\"#### Human Baseline Summary -->\")\nprint(dataset['test'][example_record]['summary'])\nprint(\"#### Summary Generated by original model->\")\nprint(original_model_output)\nprint(\"#### Summary Generated by finetuned model->\")\nprint(finetuned_model_output)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T13:33:46.653743Z","iopub.execute_input":"2024-05-01T13:33:46.654621Z","iopub.status.idle":"2024-05-01T13:33:49.171913Z","shell.execute_reply.started":"2024-05-01T13:33:46.654585Z","shell.execute_reply":"2024-05-01T13:33:49.170543Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"#### Human Baseline Summary -->\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n#### Summary Generated by original model->\n#Person1#: I'm thinking of upgrading my computer.\n#### Summary Generated by finetuned model->\n#Person2# wants to upgrade #Person2#'s system and hardware. #Person1# suggests adding a painting program to #Person2#'s software and adding a CD-ROM drive.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Evaluating the model with ROUGE & BLEU Score & compare them with the original model","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# to save time we will only use 150 items from test split for evaluation\ndialogues = final_dataset['test']['dialogue'][:150]\nprint(len(dialogues))\n\nhuman_baseline_summaries = final_dataset['test']['dialogue'][:150]\noriginal_model_summaries = []\nfinetuned_model_summaries = []\n\n# moving both models to gpu for faster inference\noriginal_model.to(\"cuda:0\")\nfinetuned_model.to(\"cuda:0\")\n\nfor dialogue in tqdm(dialogues, desc=\"Generating summaries from original & finetuned models...\"):\n    prompt = f\"\"\"\n    Summarize the following conversation.\n\n    {dialogue}\n\n    Summary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n\n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n    original_model_summaries.append(original_model_text_output)\n\n    finetuned_model_outputs = finetuned_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    finetuned_model_text_output = tokenizer.decode(finetuned_model_outputs[0], skip_special_tokens=True)\n    finetuned_model_summaries.append(finetuned_model_text_output)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T13:33:49.173327Z","iopub.execute_input":"2024-05-01T13:33:49.174197Z","iopub.status.idle":"2024-05-01T13:37:13.893404Z","shell.execute_reply.started":"2024-05-01T13:33:49.174157Z","shell.execute_reply":"2024-05-01T13:37:13.892165Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"150\n","output_type":"stream"},{"name":"stderr","text":"Generating summaries from original & finetuned models...:  10%|█         | 15/150 [00:20<02:17,  1.02s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1161 > 512). Running this sequence through the model will result in indexing errors\nGenerating summaries from original & finetuned models...: 100%|██████████| 150/150 [03:24<00:00,  1.36s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ROUGE Score","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load('rouge')\nhuman_baseline_summaries = test_summaries[:150]\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nfinetuned_model_results = rouge.compute(\n    predictions=finetuned_model_summaries,\n    references=human_baseline_summaries[0:len(finetuned_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('Finetuned MODEL:')\nprint(finetuned_model_results)\n\nrun.log({\"rouge_score\": finetuned_model_results})","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T13:37:13.894962Z","iopub.execute_input":"2024-05-01T13:37:13.895360Z","iopub.status.idle":"2024-05-01T13:37:17.655203Z","shell.execute_reply.started":"2024-05-01T13:37:13.895325Z","shell.execute_reply":"2024-05-01T13:37:17.653956Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f00f77932244318e977bffe0ee5b4b"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.23329031718657797, 'rouge2': 0.07841616534194039, 'rougeL': 0.1997259600136288, 'rougeLsum': 0.1989589635684337}\nFinetuned MODEL:\n{'rouge1': 0.45716979195215135, 'rouge2': 0.19513860068362954, 'rougeL': 0.36593641715477704, 'rougeLsum': 0.3658375336340019}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### BLEU Score","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"bleu = evaluate.load(\"bleu\")\n    \noriginal_model_results = bleu.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries\n)\n\nfinetuned_model_results = bleu.compute(\n    predictions=finetuned_model_summaries,\n    references=human_baseline_summaries,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('Finetuned MODEL:')\nprint(finetuned_model_results)\n\nrun.log({\"bleu_score\": finetuned_model_results})\n\nrun.finish()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-01T13:37:17.656892Z","iopub.execute_input":"2024-05-01T13:37:17.657303Z","iopub.status.idle":"2024-05-01T13:37:24.053566Z","shell.execute_reply.started":"2024-05-01T13:37:17.657266Z","shell.execute_reply":"2024-05-01T13:37:24.052856Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e4195b6c6541e4a5b1075394546400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2086927229c54ba9a13496974b22dd0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dbac1e3d0d34aa1a47caa74f95ee58a"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'bleu': 0.05750065583903749, 'precisions': [0.277600695450594, 0.11780738946093276, 0.060894386298763085, 0.02130492676431425], 'brevity_penalty': 0.7124595327519023, 'length_ratio': 0.7468080502055832, 'translation_length': 3451, 'reference_length': 4621}\nFinetuned MODEL:\n{'bleu': 0.21238684758776064, 'precisions': [0.4762611275964392, 0.2727966425028615, 0.17085624509033778, 0.09166329421286928], 'brevity_penalty': 1.0, 'length_ratio': 1.1668470028132438, 'translation_length': 5392, 'reference_length': 4621}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='947.604 MB of 947.604 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training time (seconds)</td><td>▁</td></tr><tr><td>eval/loss</td><td>█▃▂▁</td></tr><tr><td>eval/runtime</td><td>█▅▁▅</td></tr><tr><td>eval/samples_per_second</td><td>▁▄█▄</td></tr><tr><td>eval/steps_per_second</td><td>▁▅█▅</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▆▆███████</td></tr><tr><td>train/learning_rate</td><td>▅█▄▁</td></tr><tr><td>train/loss</td><td>█▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training time (seconds)</td><td>2206.05359</td></tr><tr><td>eval/loss</td><td>1.08435</td></tr><tr><td>eval/runtime</td><td>96.9091</td></tr><tr><td>eval/samples_per_second</td><td>14.921</td></tr><tr><td>eval/steps_per_second</td><td>0.939</td></tr><tr><td>train/epoch</td><td>0.69</td></tr><tr><td>train/global_step</td><td>1000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.2327</td></tr><tr><td>train/total_flos</td><td>5489014937223168.0</td></tr><tr><td>train/train_loss</td><td>1.32827</td></tr><tr><td>train/train_runtime</td><td>2198.6165</td></tr><tr><td>train/train_samples_per_second</td><td>14.555</td></tr><tr><td>train/train_steps_per_second</td><td>0.455</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">flant5-fullfinetune-1714568112</strong> at: <a href='https://wandb.ai/mann1105/genai-llm/runs/msfmx24f' target=\"_blank\">https://wandb.ai/mann1105/genai-llm/runs/msfmx24f</a><br/> View project at: <a href='https://wandb.ai/mann1105/genai-llm' target=\"_blank\">https://wandb.ai/mann1105/genai-llm</a><br/>Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240501_125617-msfmx24f/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Conclusion\nAs we can see that with full-finetuning we managed to get great summaries without employing few-shot learning which will help for compressing prompt without providing any proir context. ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}